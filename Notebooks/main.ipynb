{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e40879be",
      "metadata": {},
      "source": [
        "# Layer Ddefinition\n",
        "\n",
        "The layers of AvgPool2D, Convo2D, Dense, TanH, Softmax, Relu are defined here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "126fa2d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUpqgkGZpNe3",
      "metadata": {
        "id": "MUpqgkGZpNe3"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AvgPool2D:\n",
        "    def __init__(self, pool_size, stride):\n",
        "        self.ph, self.pw = pool_size\n",
        "        self.s = stride           \n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        N, C, H_in, W_in = X.shape\n",
        "        H_out = (H_in - self.ph) // self.s + 1\n",
        "        W_out = (W_in - self.pw) // self.s + 1\n",
        "        out = np.zeros((N, C, H_out, W_out))\n",
        "        for h in range(H_out):\n",
        "            for w in range(W_out):\n",
        "                h_start = h * self.s\n",
        "                h_end = h_start + self.ph\n",
        "                w_start = w * self.s\n",
        "                w_end = w_start + self.pw\n",
        "                X_slice = X[:, :, h_start:h_end, w_start:w_end]\n",
        "                out[:, :, h, w] = np.mean(X_slice, axis=(2, 3))\n",
        "        self.cache = X\n",
        "        return out\n",
        "\n",
        "    def backward(self, dOut):\n",
        "        X = self.cache\n",
        "        N, C, H_in, W_in = X.shape\n",
        "        _, _, H_out, W_out = dOut.shape\n",
        "        dX = np.zeros_like(X)\n",
        "        distribution_factor = 1.0 / (self.ph * self.pw)\n",
        "\n",
        "        for h in range(H_out):\n",
        "            for w in range(W_out):\n",
        "                h_start = h * self.s\n",
        "                h_end = h_start + self.ph\n",
        "                w_start = w * self.s\n",
        "                w_end = w_start + self.pw\n",
        "                dOut_slice = dOut[:, :, h, w, np.newaxis, np.newaxis]\n",
        "                dX_slice = dOut_slice * distribution_factor\n",
        "                dX[:, :, h_start:h_end, w_start:w_end] += dX_slice\n",
        "\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "2f38d516",
      "metadata": {},
      "outputs": [],
      "source": [
        "def im2col(X, k):\n",
        "    B, C, H, W = X.shape\n",
        "    out_h = H - k + 1\n",
        "    out_w = W - k + 1\n",
        "\n",
        "    i0 = np.repeat(np.arange(k), k)\n",
        "    i0 = np.tile(i0, C)\n",
        "    i1 = np.repeat(np.arange(out_h), out_w)\n",
        "    j0 = np.tile(np.arange(k), k * C)\n",
        "    j1 = np.tile(np.arange(out_w), out_h)\n",
        "\n",
        "    i = i0.reshape(-1,1) + i1.reshape(1,-1)\n",
        "    j = j0.reshape(-1,1) + j1.reshape(1,-1)\n",
        "\n",
        "    c = np.repeat(np.arange(C), k*k).reshape(-1,1)\n",
        "\n",
        "    cols = X[:, c, i, j]\n",
        "    return cols\n",
        "\n",
        "def col2im(cols, X_shape, k):\n",
        "    B, C, H, W = X_shape\n",
        "    out_h = H - k + 1\n",
        "    out_w = W - k + 1\n",
        "\n",
        "    X_grad = np.zeros((B, C, H, W))\n",
        "\n",
        "    i0 = np.repeat(np.arange(k), k)\n",
        "    i0 = np.tile(i0, C)\n",
        "    i1 = np.repeat(np.arange(out_h), out_w)\n",
        "    j0 = np.tile(np.arange(k), k * C)\n",
        "    j1 = np.tile(np.arange(out_w), out_h)\n",
        "\n",
        "    i = i0.reshape(-1,1) + i1.reshape(1,-1)\n",
        "    j = j0.reshape(-1,1) + j1.reshape(1,-1)\n",
        "    c = np.repeat(np.arange(C), k*k).reshape(-1,1)\n",
        "\n",
        "    np.add.at(X_grad, (slice(None), c, i, j), cols.reshape(B, -1, out_h*out_w))\n",
        "\n",
        "    return X_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "nxtFxs6tpOeN",
      "metadata": {
        "id": "nxtFxs6tpOeN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Conv2D:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        self.in_ch = in_channels\n",
        "        self.out_ch = out_channels\n",
        "        self.k = kernel_size\n",
        "\n",
        "        limit = np.sqrt(1 / (in_channels * kernel_size * kernel_size))\n",
        "        self.params = {\n",
        "            \"W\": np.random.uniform(-limit, limit,\n",
        "                (out_channels, in_channels, kernel_size, kernel_size)),\n",
        "            \"b\": np.zeros(out_channels)\n",
        "        }\n",
        "        self.grads = {\n",
        "            \"W\": np.zeros_like(self.params[\"W\"]),\n",
        "            \"b\": np.zeros_like(self.params[\"b\"])\n",
        "        }\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = np.ascontiguousarray(X)\n",
        "        self.X = X\n",
        "        B, C, H, W = X.shape\n",
        "        k = self.k\n",
        "        self.out_h = H - k + 1\n",
        "        self.out_w = W - k + 1\n",
        "\n",
        "        self.X_col = im2col(X, k)               \n",
        "        self.X_col = self.X_col.transpose(0, 2, 1)  \n",
        "        W_col = self.params['W'].reshape(self.out_ch, -1)\n",
        "\n",
        "        out = self.X_col @ W_col.T\n",
        "        out = out.transpose(0,2,1).reshape(B, self.out_ch, self.out_h, self.out_w)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        X = np.ascontiguousarray(self.X)\n",
        "        B = dout.shape[0]\n",
        "        k = self.k\n",
        "        C_out = self.out_ch\n",
        "\n",
        "        dout_flat = dout.reshape(B, C_out, -1).transpose(0,2,1)\n",
        "\n",
        "        dW_col = dout_flat.transpose(0,2,1) @ self.X_col \n",
        "        self.grads[\"W\"] = dW_col.sum(axis=0).reshape(self.params[\"W\"].shape)\n",
        "\n",
        "        self.grads[\"b\"] = dout.sum(axis=(0,2,3))\n",
        "\n",
        "        W_col = self.params[\"W\"].reshape(C_out, -1)\n",
        "        dX_col = dout_flat @ W_col \n",
        "        dX_col = dX_col.transpose(0,2,1)\n",
        "\n",
        "        dX = col2im(dX_col, self.X.shape, k)\n",
        "        return dX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "0ucp3HVtpSZ1",
      "metadata": {
        "id": "0ucp3HVtpSZ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, in_features, out_features):\n",
        "        limit = np.sqrt(1 / in_features)\n",
        "        self.params = {}\n",
        "        self.params['W'] = np.random.uniform(-limit, limit, (in_features, out_features))\n",
        "        self.params['b'] = np.zeros((1, out_features))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X        \n",
        "        return X @ self.params['W'] + self.params['b']\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        self.grads = {}\n",
        "        self.grads['W'] = self.X.T @ d_out\n",
        "        self.grads['b'] = np.sum(d_out, axis=0, keepdims=True)\n",
        "        return d_out @ self.params['W'].T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "Eo_cTnbOpVCz",
      "metadata": {
        "id": "Eo_cTnbOpVCz"
      },
      "outputs": [],
      "source": [
        "class Flatten:\n",
        "    def forward(self, X):\n",
        "        self.shape = X.shape\n",
        "        return X.reshape(X.shape[0], -1)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        return d_out.reshape(self.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "RLGZaLF7pYG8",
      "metadata": {
        "id": "RLGZaLF7pYG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        return d_out * (self.X > 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "EdrOvkMcpZ_s",
      "metadata": {
        "id": "EdrOvkMcpZ_s"
      },
      "outputs": [],
      "source": [
        "class Sequence:\n",
        "    def __init__(self, layers, loss_fn=None, optimizer=None):\n",
        "        self.layers = layers\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = X\n",
        "        for layer in self.layers:\n",
        "            out = layer.forward(out)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, y):\n",
        "        loss = self.loss_fn.forward(self.out, y)\n",
        "        grad = self.loss_fn.backward()\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "        return loss\n",
        "\n",
        "    def step(self):\n",
        "        if self.optimizer is None:\n",
        "            return\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, \"params\"):\n",
        "                for name in layer.params:\n",
        "                    self.optimizer.update(\n",
        "                        layer.params[name],\n",
        "                        layer.grads[name]\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "kDbkT2h0pbpj",
      "metadata": {
        "id": "kDbkT2h0pbpj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        shifted = X - np.max(X, axis=1, keepdims=True)\n",
        "        exp = np.exp(shifted)\n",
        "        self.out = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, d_out,*args):\n",
        "        B, C = d_out.shape\n",
        "        dX = np.zeros_like(d_out)\n",
        "\n",
        "        for b in range(B):\n",
        "            s = self.out[b].reshape(-1, 1)          # (C,1)\n",
        "            jac = np.diagflat(s) - (s @ s.T)        # (C,C)\n",
        "            dX[b] = jac @ d_out[b]\n",
        "\n",
        "        return dX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "NGsn-sjapdOZ",
      "metadata": {
        "id": "NGsn-sjapdOZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.out = np.tanh(X)\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, d_out):\n",
        "        return d_out * (1 - self.out ** 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50325a5",
      "metadata": {},
      "source": [
        "# LossFunctions and Optimizers defined\n",
        "\n",
        "Here Loss Function Softmax Cross Entropy is definied, and the optimizer of SGD is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "wKxC01h6pmcR",
      "metadata": {
        "id": "wKxC01h6pmcR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SoftmaxCrossEntropy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, logits, y_true):\n",
        "        self.logits = logits\n",
        "        B, C = logits.shape\n",
        "        shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
        "        exp = np.exp(shifted)\n",
        "        self.probs = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        log_likelihood = -np.log(self.probs[np.arange(B), y_true] + 1e-12)\n",
        "        loss = np.mean(log_likelihood)\n",
        "        self.y_true = y_true\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        B = self.logits.shape[0]\n",
        "        grad = self.probs.copy()\n",
        "        grad[np.arange(B), self.y_true] -= 1\n",
        "        return grad / B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "bFib-5Jzpoe3",
      "metadata": {
        "id": "bFib-5Jzpoe3"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, param, grad):\n",
        "        param -= self.lr * grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6c7a1c",
      "metadata": {},
      "source": [
        "# Model \n",
        "\n",
        "The model implemented below follows the LeNet-5 convolutional neural network proposed by LeCun et al. in their 1998 paper on handwritten digit recognition.\n",
        "\n",
        "The model implemented here however has some changes according to convinence. \n",
        "\n",
        "1. The original model used partial connections in C3, which is avoided here. \n",
        "\n",
        "1. The original model was trained till convergence, here it is trained till 40 epochs, which is a lot, but with SGD it isn't enough to converge. \n",
        "\n",
        "1. The original model used the subsampling layer with learnable parameters, which was avoided here by using simple AvgPooling\n",
        "\n",
        "1. The original model used a variant of MSE + RBF style activation, but CrossEntropy is Used here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "pce_SI3TptYS",
      "metadata": {
        "id": "pce_SI3TptYS"
      },
      "outputs": [],
      "source": [
        "class Lenet5(Sequence):\n",
        "    def __init__(self):\n",
        "        layers = [\n",
        "            Conv2D(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            Tanh(),\n",
        "\n",
        "            AvgPool2D((2,2),2),\n",
        "            Tanh(),\n",
        "\n",
        "            Conv2D(in_channels=6 ,out_channels=16, kernel_size=5),\n",
        "            Tanh(),\n",
        "\n",
        "            AvgPool2D((2,2),2),\n",
        "            Tanh(),\n",
        "\n",
        "            Conv2D(in_channels=16, out_channels=120, kernel_size=5),\n",
        "            Tanh(),\n",
        "\n",
        "            Flatten(),\n",
        "            \n",
        "            DenseLayer(120, 84),\n",
        "            Tanh(),\n",
        "\n",
        "            DenseLayer(84, 10)\n",
        "        ]\n",
        "        super().__init__(layers=layers,loss_fn=SoftmaxCrossEntropy(),optimizer=SGD())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e8c752",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "1. Loading Dataset\n",
        "\n",
        "1. Evaluation of Model\n",
        "\n",
        "1. Getting Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "3c187ee5",
      "metadata": {
        "id": "3c187ee5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_mnist():\n",
        "    mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
        "\n",
        "    X = mnist.data.astype(np.float32) / 255.0   # normalize\n",
        "    y = mnist.target.astype(np.int64)\n",
        "    X = X.reshape(-1, 1, 28, 28)\n",
        "    X_padded = np.pad(X, ((0,0), (0,0), (2,2), (2,2)), mode='constant')\n",
        "    X_train, X_test = X_padded[:10000], X_padded[68000:]\n",
        "    y_train, y_test = y[:10000], y[68000:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "490663ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate(model, X, y, batch_size=128):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size]\n",
        "        yb = y[i:i+batch_size]\n",
        "        logits = model.forward(xb)\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        correct += np.sum(preds == yb)\n",
        "        total += len(yb)\n",
        "\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "07db870b",
      "metadata": {
        "id": "07db870b"
      },
      "outputs": [],
      "source": [
        "def get_batches(X, y, batch_size):\n",
        "    idx = np.random.permutation(len(X))\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        yield X[i:i+batch_size], y[i:i+batch_size]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd993d8",
      "metadata": {},
      "source": [
        "# Main Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "dadbe245",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "8008c408",
      "metadata": {
        "id": "8008c408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (10000, 1, 32, 32) (10000,)\n",
            "Test: (2000, 1, 32, 32) (2000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Test:\",  X_test.shape,  y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "312f6f41",
      "metadata": {
        "id": "312f6f41"
      },
      "outputs": [],
      "source": [
        "model = Lenet5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "5d4c6b32",
      "metadata": {
        "id": "5d4c6b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 2.2845, | Acc: 0.2607\n",
            "Epoch 2 | Loss: 2.2800, | Acc: 0.369\n",
            "Epoch 3 | Loss: 2.2570, | Acc: 0.4296\n",
            "Epoch 4 | Loss: 2.2587, | Acc: 0.4651\n",
            "Epoch 5 | Loss: 2.2051, | Acc: 0.4731\n",
            "Epoch 6 | Loss: 2.1340, | Acc: 0.4965\n",
            "Epoch 7 | Loss: 2.1112, | Acc: 0.5271\n",
            "Epoch 8 | Loss: 1.8623, | Acc: 0.5329\n",
            "Epoch 9 | Loss: 1.7035, | Acc: 0.5441\n",
            "Epoch 10 | Loss: 1.7427, | Acc: 0.5881\n",
            "Epoch 11 | Loss: 1.2500, | Acc: 0.6391\n",
            "Epoch 12 | Loss: 1.0329, | Acc: 0.6902\n",
            "Epoch 13 | Loss: 1.0878, | Acc: 0.7211\n",
            "Epoch 14 | Loss: 0.5486, | Acc: 0.7441\n",
            "Epoch 15 | Loss: 0.8916, | Acc: 0.7644\n",
            "Epoch 16 | Loss: 0.9002, | Acc: 0.7788\n",
            "Epoch 17 | Loss: 0.6481, | Acc: 0.7907\n",
            "Epoch 18 | Loss: 0.4694, | Acc: 0.8024\n",
            "Epoch 19 | Loss: 0.5099, | Acc: 0.812\n",
            "Epoch 20 | Loss: 0.8264, | Acc: 0.8204\n",
            "Epoch 21 | Loss: 1.2078, | Acc: 0.8334\n",
            "Epoch 22 | Loss: 0.5243, | Acc: 0.8375\n",
            "Epoch 23 | Loss: 0.6102, | Acc: 0.8434\n",
            "Epoch 24 | Loss: 0.5067, | Acc: 0.8487\n",
            "Epoch 25 | Loss: 0.5669, | Acc: 0.8545\n",
            "Epoch 26 | Loss: 0.3730, | Acc: 0.862\n",
            "Epoch 27 | Loss: 0.5827, | Acc: 0.8657\n",
            "Epoch 28 | Loss: 0.3133, | Acc: 0.87\n",
            "Epoch 29 | Loss: 0.5154, | Acc: 0.8726\n",
            "Epoch 30 | Loss: 0.2699, | Acc: 0.877\n",
            "Epoch 31 | Loss: 0.9338, | Acc: 0.8792\n",
            "Epoch 32 | Loss: 0.3832, | Acc: 0.8834\n",
            "Epoch 33 | Loss: 0.3430, | Acc: 0.8874\n",
            "Epoch 34 | Loss: 0.4925, | Acc: 0.8895\n",
            "Epoch 35 | Loss: 0.4699, | Acc: 0.8914\n",
            "Epoch 36 | Loss: 0.7487, | Acc: 0.8946\n",
            "Epoch 37 | Loss: 0.2150, | Acc: 0.8945\n",
            "Epoch 38 | Loss: 0.3176, | Acc: 0.898\n",
            "Epoch 39 | Loss: 0.1923, | Acc: 0.8993\n",
            "Epoch 40 | Loss: 0.0768, | Acc: 0.9005\n"
          ]
        }
      ],
      "source": [
        "epochs = 40\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss = 0\n",
        "    for Xb, yb in get_batches(X_train, y_train, batch_size):\n",
        "        logits = model.forward(Xb)\n",
        "        loss = model.backward(yb)\n",
        "        model.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f}, | Acc: {evaluate(model,X_train,y_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "lw85UipRfJN_",
      "metadata": {
        "id": "lw85UipRfJN_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.928\n"
          ]
        }
      ],
      "source": [
        "acc = evaluate(model, X_test, y_test)\n",
        "print(\"Test Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f90f2e",
      "metadata": {},
      "source": [
        "# Conclusions\n",
        "\n",
        "The model here reached almost 90~ accuracy on test set as compared to 99.2% of original model. This is due to the model was trained till 40 epochs only and on a smaller subset of MNIST dataset due to compute contraints. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
